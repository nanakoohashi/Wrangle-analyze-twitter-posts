#!/usr/bin/env python
# coding: utf-8

# # Gathering Data

# In[1]:


import tweepy
from tweepy import OAuthHandler
import json
from timeit import default_timer as timer
import pandas as pd
import requests
import os


# In[2]:


# read csv file
df = pd.read_csv('twitter-archive-enhanced.csv')


# In[3]:


# create copy of df
df_doggo = df.copy()


# In[9]:


df_doggo.head(20)



# In[4]:


folder_name = 'image_predictions'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)


# In[5]:


image_prediction_urls= [' https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv']


# In[10]:


# Programmatically download file from website
for url in image_prediction_urls:
    response = requests.get(url)
    with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as file:
        file.write(response.content)


# In[11]:

# Read and view df_image
df_i = pd.read_csv('image-predictions (1).tsv', sep='\t')



# In[ ]:

# Make a copy of df_i
df_image = df_i.copy()



# In[12]:


# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file
# These are hidden to comply with Twitter's API terms and conditions
consumer_key = ''
consumer_secret = ''
access_token = ''
access_secret = ''

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)

api = tweepy.API(auth, wait_on_rate_limit=True)

tweet_ids = df.tweet_id.values
len(tweet_ids)

# Query Twitter's API for JSON data for each tweet ID in the Twitter archive
count = 0
fails_dict = {}
start = timer()
# Save each tweet's returned JSON as a new line in a .txt file
with open('tweet_json.txt', 'w') as outfile:
    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit
    for tweet_id in tweet_ids:
        count += 1
        print(str(count) + ": " + str(tweet_id))
        try:
            tweet = api.get_status(tweet_id, tweet_mode='extended')
            print("Success")
            json.dump(tweet._json, outfile)
            outfile.write('\n')
        except tweepy.TweepError as e:
            print("Fail")
            fails_dict[tweet_id] = e
            pass
end = timer()
print(end - start)
print(fails_dict)


# In[13]:

# Create new data frame with id, retweet_count, and favorite_count

tweet_json = open('tweet_json.txt', 'r')
df_tweet = pd.DataFrame(columns=['id', 'retweets', 'favorites'])

for line in tweet_json:
    tweet = json.loads(line)
    df_tweet = df_tweet_data.append({'id': tweet['id'], 'retweets': tweet['retweet_count'], 'favorites': tweet['favorite_count']}, ignore_index=True)
tweet_json.close()

df_tweet


# In[ ]:


# Make a copy of df_tweet
df_tweet_data = df_tweet.copy()



# # Assessing Data


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_doggo.info()


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_image.info()


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_tweet_data.info()


# In[ ]:

# Make sure numerators are consistent.
df_doggo.rating_numerator.unique()


# In[ ]:

# Check for any numerators that equal zero.
df_doggo[df_doggo.rating_numerator == 0]


# In[ ]:

# Make sure denominators are consistent
df_doggo.rating_denominator.unique()


# In[ ]:

# Check for any denominators that equal zero.
df_doggo[df_doggo.rating_denominator == 0]


# In[ ]:

# Detect entries where there are more than one dog stage.
df_doggo.loc[(df_doggo[['doggo', 'floofer', 'pupper', 'puppo']] != 'None').sum(axis=1) > 1]


# In[14]:

# Convert df_tweet_data from str to int.
df_tweet_data = df_tweet_data.astype(str).astype(int)


# In[15]:

# Check if there are any favorites values that equal 0 in df_new.
df_tweet_data[df_tweet_data['favorites'] == 0]


# In[16]:

# Find retweets that are more than favorites in df_new.
df_tweet_data[df_tweet_data['retweets'] > df_tweet_data['favorites']]


# In[17]:

# Find retweets that equal 0 in df_new.
df_tweet_data[df_tweet_data['retweets'] == 0]

