#!/usr/bin/env python
# coding: utf-8

# # Gathering Data

# In[1]:


import tweepy
from tweepy import OAuthHandler
import json
from timeit import default_timer as timer
import pandas as pd
import requests
import os


# In[2]:


# read csv file
df = pd.read_csv('twitter-archive-enhanced.csv')


# In[3]:


# create copy of df
df_doggo = df.copy()


# In[9]:


df_doggo.head(20)



# In[4]:


folder_name = 'image_predictions'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)


# In[5]:


image_prediction_urls= [' https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv']


# In[10]:


# Programmatically download file from website
for url in image_prediction_urls:
    response = requests.get(url)
    with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as file:
        file.write(response.content)


# In[11]:

# Read and view df_image
df_i = pd.read_csv('image-predictions (1).tsv', sep='\t')



# In[ ]:

# Make a copy of df_i
df_image = df_i.copy()



# In[12]:


# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file
# These are hidden to comply with Twitter's API terms and conditions
consumer_key = ''
consumer_secret = ''
access_token = ''
access_secret = ''

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)

api = tweepy.API(auth, wait_on_rate_limit=True)

tweet_ids = df.tweet_id.values
len(tweet_ids)

# Query Twitter's API for JSON data for each tweet ID in the Twitter archive
count = 0
fails_dict = {}
start = timer()
# Save each tweet's returned JSON as a new line in a .txt file
with open('tweet_json.txt', 'w') as outfile:
    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit
    for tweet_id in tweet_ids:
        count += 1
        print(str(count) + ": " + str(tweet_id))
        try:
            tweet = api.get_status(tweet_id, tweet_mode='extended')
            print("Success")
            json.dump(tweet._json, outfile)
            outfile.write('\n')
        except tweepy.TweepError as e:
            print("Fail")
            fails_dict[tweet_id] = e
            pass
end = timer()
print(end - start)
print(fails_dict)


# In[13]:

# Create new data frame with id, retweet_count, and favorite_count

tweet_json = open('tweet_json.txt', 'r')
df_tweet = pd.DataFrame(columns=['id', 'retweets', 'favorites'])

for line in tweet_json:
    tweet = json.loads(line)
    df_tweet = df_tweet_data.append({'id': tweet['id'], 'retweets': tweet['retweet_count'], 'favorites': tweet['favorite_count']}, ignore_index=True)
tweet_json.close()

df_tweet


# In[ ]:


# Make a copy of df_tweet
df_tweet_data = df_tweet.copy()



# # Assessing Data


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_doggo.info()


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_image.info()


# In[ ]:

# Check data types are compatible and columns are not missing entries.
df_tweet_data.info()


# In[ ]:

# Make sure numerators are consistent.
df_doggo.rating_numerator.unique()


# In[ ]:

# Check for any numerators that equal zero.
df_doggo[df_doggo.rating_numerator == 0]


# In[ ]:

# Make sure denominators are consistent
df_doggo.rating_denominator.unique()


# In[ ]:

# Check for any denominators that equal zero.
df_doggo[df_doggo.rating_denominator == 0]


# In[ ]:

# Detect entries where there are more than one dog stage.
df_doggo.loc[(df_doggo[['doggo', 'floofer', 'pupper', 'puppo']] != 'None').sum(axis=1) > 1]


# In[14]:

# Convert all entries in df_tweet_data to integer to help assess data further.
df_tweet_data = df_tweet_data.astype(str).astype(int)


# In[15]:

# Check if there are any favorites values that equal 0 in df_new.
df_tweet_data[df_tweet_data['favorites'] == 0]


# In[16]:

# Find retweets that are more than favorites in df_new.
df_tweet_data[df_tweet_data['retweets'] > df_tweet_data['favorites']]


# In[17]:

# Find retweets that equal 0 in df_new.
df_tweet_data[df_tweet_data['retweets'] == 0]


# In[ ]:

# See if there are any duplicates in the data frame.
sum(df_tweet_data.duplicated())


# In[ ]:

# See if there are any duplicates in the data frame.
sum(df_image.duplicated())


# In[ ]:

# See if there are any duplicates in the data frame.
sum(df_doggo.duplicated())

# ## Detect and document at least eight (8) quality issues and two (2) tidiness issues
# Quality issue dimensions are:
# 1. `Completeness`: do we have all of the records that we should? Do we have missing records or not? Are there specific rows, columns, or cells missing?
# 2. `Validity`: we have the records, but they’re not valid, i.e., they don’t conform to a defined schema. A schema is a defined set of rules for data. These rules can be real-world constraints (e.g. negative height is impossible) and table-specific constraints (e.g. unique key constraints in tables).
# 3. `Accuracy`: inaccurate data is wrong data that is valid. It adheres to the defined schema, but it is still incorrect. Example: a patient’s weight that is 5 lbs too heavy because the scale was faulty.
# 4. `Consistency`: inconsistent data is both valid and accurate, but there are multiple correct ways of referring to the same thing. Consistency, i.e., a standard format, in columns that represent the same data across tables and/or within tables is desired.
# 
# Tidy Data requirements:
# 1. Each variable forms a column.
# 2. Each observation forms a row.
# 3. Each type of observational unit forms a table.
# 
# ### Quality Issues
# 
# 1. Missing values for dog stage (incomplete data for doggo, floofer, pupper, puppo) in `df_doggo`.
# 2. Multiple values for dog stage in `df_doggo`.
# 3. Rating numerators and  rating denominators values are incorrect in `df_doggo`.
# 4. Remove entries that are retweets in `df_doggo`.
# 5. Replace 'None' with 'NaN' for all dog stages in `df_doggo`.
# 6. Columns pertaining to retweets and expanded URLs are unnecessary in `df_doggo`.
# 7. Remove entries where p1_dog, p2_dog, and p3_dog are all "False" in `df_image`.
# 8. Values for p1, p2, and p3 sometimes capitalized but not always in `df_image`.
# 9. All columns in `df_tweet_data` should be integers.
# 10. 167 rows missing data for `favorites` in `df_tweet_data`.
# 
# 
# ### Tidiness Issues
# 1. Doggo, floofer, pupper, puppo are one variable spread across different columns in `df_doggo`. 
# 2. rating_numerator and rating_denominator can be combined into one column in `df_doggo`.
# 3. Combine data frames by tweet_id.
# 
# # Cleaning Data

# ## Quality Issues

# ### 1. Missing values for dog stage (incomplete data for `doggo`, `floofer`, `pupper`, `puppo`) in `df_doggo`.
# - Remove rows that contain "None" values for all dog stages.

# #### Code

# In[25]:


df_doggo_1 = df_doggo.query('doggo != "None" or floofer != "None" or pupper != "None" or puppo != "None"')


# #### Test

# In[26]:


df_doggo_1


# ### 2. Multiple values for dog stage in `df_doggo`.

# - Remove rows that contain multiple values for all dog stages.

# #### Code

# In[27]:


# Remove entries where there are more than one dog stage
df_doggo_1 = df_doggo_1.loc[(df_doggo_1[['doggo', 'floofer', 'pupper', 'puppo']] != 'None').sum(axis=1) == 1]


# #### Test

# In[28]:


df_doggo_1.loc[(df_doggo_1[['doggo', 'floofer', 'pupper', 'puppo']] != 'None').sum(axis=1) > 1]



# ### 3. `rating_numerators` and  `rating_denominator` values are incorrect in `df_doggo`.

# #### Code

# In[29]:


df_doggo_1['rating_denominator'].unique()


# In[30]:


df_doggo_1['rating_numerator'].unique()


# In[31]:


df_doggo_1['rating_numerator'] = df_doggo_1.text.str.extract('((?:\d+\.)?\d+)\/(\d+)', expand=True)


# In[32]:


df_doggo_1['rating_numerator'] = df_doggo_1['rating_numerator'].astype(str).astype(float)


# In[33]:


df_doggo_1['rating_denominator'] = df_doggo_1['rating_denominator'].astype(str).astype(float)


# #### Test

# In[34]:


df_doggo_1['rating_denominator'].unique()


# In[35]:


df_doggo_1['rating_numerator'].unique()


# In[36]:


# Make sure that numerator and denominator are in compatible format
df_doggo_1['rating_numerator'] / df_doggo_1['rating_denominator']



# ### 4. Remove entries that are retweets in `df_doggo`.

# #### Code

# In[37]:


df_doggo_2 = df_doggo_1.query('retweeted_status_id == "NaN"')


# #### Test

# In[38]:


# Test to see if retweet columns contain any values.

df_doggo_2.info()



# ### 5. Replace 'None' with 'NaN' for all dog stages in `df_doggo`.

# #### Code

# In[39]:


df_doggo_2['doggo'] = np.where(df_doggo_2['doggo'] == 'None' , np.nan, df_doggo_2['doggo'])
df_doggo_2['floofer'] = np.where(df_doggo_2['floofer'] == 'None' , np.nan, df_doggo_2['floofer'])
df_doggo_2['pupper'] = np.where(df_doggo_2['pupper'] == 'None' , np.nan, df_doggo_2['pupper'])
df_doggo_2['puppo'] = np.where(df_doggo_2['puppo'] == 'None' , np.nan, df_doggo_2['puppo'])


# #### Test

# In[40]:


df_doggo_2



# ### 6. Columns pertaining to retweets and expanded URLs are unnecessary in df_doggo.

# #### Code

# In[41]:


df_doggo_2.drop(columns=['retweeted_status_id','retweeted_status_user_id', 'retweeted_status_timestamp', 'expanded_urls', 'source'], inplace=True)


# #### Test

# In[42]:


df_doggo_2


